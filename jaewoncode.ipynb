{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jaewoncode.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from os import defpath\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pandas.core.frame import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "VgThywZ0SXJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def strToScaleTime(str):\n",
        "    timestamp = datetime.strptime(str,\"%Y-%m-%d %H:%M\")\n",
        "    return timestamp.hour/24 + timestamp.minute/(24*60)\n"
      ],
      "metadata": {
        "id": "fhw-gt6MSYUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def samplingData(X_train,y_train):\n",
        "    y_classes,_ = np.unique(y_train, return_counts = True)\n",
        "    SMOTE_dict = {}\n",
        "    for y_class in y_classes: SMOTE_dict[y_class] = int(len(y_train)/len(y_classes))\n",
        "    over = SMOTE(SMOTE_dict)\n",
        "    under = RandomUnderSampler()\n",
        "    steps = [('u',under),('o', over)]\n",
        "    pipeline = Pipeline(steps=steps)\n",
        "    X_train, y_train = pipeline.fit_resample(X_train, y_train)\n",
        "    return X_train, y_train"
      ],
      "metadata": {
        "id": "PIMd-2g9SYam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scalingData(X,userName,save=True):\n",
        "    scaler = StandardScaler(copy=True, with_mean=True,with_std=True)\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    if save == True:\n",
        "        if not os.path.isfile(f'./EvaluateModel/scaler_{userName}.pickle'):\n",
        "            with open(f'./EvaluateModel/scaler_{userName}.pickle', 'wb') as f: \n",
        "                pickle.dump(scaler, f, pickle.HIGHEST_PROTOCOL)\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "WixGPEnDSYge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def importXYdataset(userName, crossValidate = 5, heartrate= True, all=False, sampling=True,scaling=True):\n",
        "    '''\n",
        "    \n",
        "    userName: Choi heeju / Choi Jinwoo / Kong minjin / Seungkeun / Hyuna Kang / Juwon / Hakpyeong Kim / Jongbaek\n",
        "    crossValidate: How many fold?\n",
        "    return: X, Y, X_trainSet, X_testSet, y_trainSet, y_testSet\n",
        "    you can print number of labels data by\n",
        "    print(dataset['ThermalSensation'].value_counts())\n",
        "    \n",
        "    '''\n",
        "    print(\"=\"*30)\n",
        "    print(f\"User name: {userName}\")\n",
        "    print(f\"Heartrate: {heartrate}\")\n",
        "    \n",
        "\n",
        "    dataset = pd.read_excel('../integrateResult.xlsx',index_col=0)\n",
        "    dataset = dataset.copy()\n",
        "    dataset = dataset.drop(['Cloth','ThermalComfort'],axis=1)\n",
        "    englishNameList = pd.read_excel(\"../CONFIG.xlsx\",index_col=0)['names'].to_list()\n",
        "    \n",
        "    # 예외처리 1: OutCloth에 '-'이 있음 : 행에서 직접변경 840 - 869\n",
        "    if all == True: dataset = dataset[dataset['Name'].isin(englishNameList)].dropna().reset_index(drop=True)\n",
        "    else:           dataset = dataset[dataset['Name']==userName].dropna().reset_index(drop=True)\n",
        "    dataset['Time'] = dataset['Time'].apply(lambda x : strToScaleTime(x))\n",
        "    \n",
        "    \n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in range(len(dataset)):\n",
        "        if heartrate == True: X.append(dataset.loc[i,['Time','OutCloth','Temperature','Humidity','Heartrate']].to_list())\n",
        "        else:                 X.append(dataset.loc[i,['Time','OutCloth','Temperature','Humidity']].to_list())\n",
        "        \n",
        "        # 예외처리 2: Choi heeju에서 thermal sensation이 공백으로 찍힘\n",
        "        if dataset.loc[i,'ThermalSensation'] == ' ': Y.append(0)\n",
        "        else: Y.append(dataset.loc[i,'ThermalSensation'])\n",
        "\n",
        "    print('Num data: {}'.format(len(Y)))\n",
        "    \n",
        "    ####### scaler\n",
        "    if scaling == True: X = scalingData(X,userName,save=True)\n",
        "    \n",
        "    X_trainSet, X_testSet, y_trainSet, y_testSet = [], [], [], []\n",
        "    skf = StratifiedKFold(n_splits=crossValidate, shuffle=True)\n",
        "    for train_index, test_index in skf.split(X, Y):\n",
        "        X_train   = [X[i] for i in train_index]\n",
        "        X_test    = [X[i] for i in test_index]\n",
        "        y_train   = [Y[i] for i in train_index]\n",
        "        y_test    = [Y[i] for i in test_index]\n",
        "        \n",
        "        y_classes,_ = np.unique(y_train, return_counts = True)\n",
        "        \n",
        "        ####### sampling\n",
        "        if sampling == True: X_train, y_train = samplingData(X_train,y_train)\n",
        "\n",
        "        \n",
        "        \n",
        "        X_trainSet.append(X_train)\n",
        "        X_testSet.append(X_test)\n",
        "        y_trainSet.append(y_train)\n",
        "        y_testSet.append(y_test)\n",
        "        \n",
        "          \n",
        "    return X,Y, X_trainSet, X_testSet, y_trainSet, y_testSet\n"
      ],
      "metadata": {
        "id": "njLBHkYQSqPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    englishNameList = pd.read_excel(\"../CONFIG.xlsx\",index_col=0)['names'].to_list()\n",
        "    importXYdataset(\"Juwon\", crossValidate = 5, heartrate= False, all=False, sampling=True,scaling=True)    \n",
        "    # for userName in englishNameList :\n",
        "    #     importXYdataset(userName, crossValidate = 5, heartrate= False, all=False, sampling=True,scaling=True)    \n",
        "    # pass"
      ],
      "metadata": {
        "id": "J7_gmOHkSqbp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}